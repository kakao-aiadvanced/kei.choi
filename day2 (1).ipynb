{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JiLbqRPFRo6O"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchainhub langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FlY3L1AaUYIg"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4o-mini')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG4pmppJdTxG",
        "outputId": "38ea8571-f44c-4a75-a9e0-5f677a052a81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qirl85w721E7"
      },
      "source": [
        "목적별로 이런식으로 prompt를 작성하면 좋겠다\n",
        "이런 것들을 모아둔것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPyuq2YQ20i0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Ql0mZy7hZl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "ZzCRwH3ob8J-",
        "outputId": "13180dab-28fb-4226-cc86-7fd2ad6cd885"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Task Decomposition is the process of breaking down complex tasks into smaller, manageable steps to enhance understanding and execution. It often involves techniques like Chain of Thought (CoT) prompting, where models think step-by-step, and Tree of Thoughts, which explores multiple reasoning paths at each step. This approach can be facilitated by simple prompting, task-specific instructions, or human input.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTi0oWZ3fWko",
        "outputId": "32fe59a7-8222-493e-b28b-a0818a5ad676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-quAwEi87OTY"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "loader = TextLoader(\"/content/drive/MyDrive/Untitled Folder/state_of_the_union.txt\")\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptyCEsGTfUKj",
        "outputId": "fa46bb88-9db6-4979-ae12-22d96222923f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x792227802050>, search_kwargs={'filter': {'paper_title': 'GPT-4 Technical Report'}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve more documents with higher diversity\n",
        "# Useful if your dataset has many similar documents\n",
        "vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
        ")\n",
        "\n",
        "# Fetch more documents for the MMR algorithm to consider\n",
        "# But only return the top 5\n",
        "vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
        ")\n",
        "\n",
        "# Only retrieve documents that have a relevance score\n",
        "# Above a certain threshold\n",
        "vectorstore.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={'score_threshold': 0.8}\n",
        ")\n",
        "\n",
        "# Only get the single most similar document from the dataset\n",
        "vectorstore.as_retriever(search_kwargs={'k': 1})\n",
        "\n",
        "# Use a filter to only retrieve documents from a specific paper\n",
        "vectorstore.as_retriever(\n",
        "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmH5BecjgA9d",
        "outputId": "5a94439c-b0af-46f5-9962-fc1b97afbeb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "joke_query = \"Tell me a joke.\"\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "chain.invoke({\"query\": joke_query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK9wzpahg9C3",
        "outputId": "3a5276e6-9306-48dd-aa54-f6eff7cf5951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"definition\": \"Task decomposition is the process of breaking down a complex task or problem into smaller, more manageable sub-tasks or components. This approach helps in simplifying the overall task, making it easier to understand, execute, and manage.\",\n",
            "  \"benefits\": [\n",
            "    \"Enhances clarity and understanding of tasks\",\n",
            "    \"Facilitates better planning and resource allocation\",\n",
            "    \"Allows for parallel execution of sub-tasks\",\n",
            "    \"Improves problem-solving by isolating specific issues\",\n",
            "    \"Helps in tracking progress and identifying bottlenecks\"\n",
            "  ],\n",
            "  \"applications\": [\n",
            "    \"Project management\",\n",
            "    \"Software development\",\n",
            "    \"Research and analysis\",\n",
            "    \"Education and learning strategies\",\n",
            "    \"Personal productivity and goal setting\"\n",
            "  ]\n",
            "}\n",
            "```"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7qDUHRgq_Xg",
        "outputId": "0bf8014b-cb3c-4c7b-a5aa-9c0f561a3ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'relevance': 'no'}\n",
            "{'relevance': 'yes'}\n",
            "{'relevance': 'yes'}\n"
          ]
        }
      ],
      "source": [
        "parser = JsonOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "            You are an AI language model assistant. check the relevance between documents given and query<|eot_id|>\\n\n",
        "            <|start_header_id|>user<|end_header_id|>\\n{format_instructions}\\n{query}\\n<|eot_id|>\\n\n",
        "            <|start_header_id|>documents<|end_header_id|>{documents}<|eot_id|>\\n\n",
        "            <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"query\",\"documents\",\"format_instructions\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()+\"\"\"For example, if there is relevance then just return {\"relevance\": \"yes\"} otherwise return {\"relevance\": \"no\"}. just a json object not any description\"\"\"},\n",
        ")\n",
        "\n",
        "chain = {\"documents\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | parser #StrOutputParser()\n",
        "\n",
        "print(chain.invoke(\"What is Task Decomposition?\"))\n",
        "\n",
        "print(chain.invoke(\"With a duty to one another to the American people to the Constitution\"))\n",
        "\n",
        "print(chain.invoke(\"Did Putin meet with Ukrainian people?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJHsZbBTsN6A",
        "outputId": "9d4caf42-c05b-455f-a408-3d2ebc82d696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"hallucination\":\"no\"}"
          ]
        }
      ],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "            You are an AI language model assistant. check the relevance between documents given and query<|eot_id|>\\n\n",
        "            <|start_header_id|>user<|end_header_id|>\\n{format_instructions}\\n{query}\\n<|eot_id|>\\n\n",
        "            <|start_header_id|>documents<|end_header_id|>{documents}<|eot_id|>\\n\n",
        "            <|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"query\",\"documents\",\"format_instructions\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()+\"\"\"For example, if there is hallucination then just return {\"hallucination\": \"yes\"} otherwise return {\"hallucination\": \"no\"}. just a json object not any description\"\"\"},\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"documents\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"putin is russian\"):\n",
        "    print(chunk, end=\"\", flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
